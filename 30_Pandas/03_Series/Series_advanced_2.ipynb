{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Series — Expert Practice (with Solutions)\n",
    "\n",
    "This notebook is **expert level** practice on `pandas.Series`.\n",
    "\n",
    "**What makes these “expert”:**\n",
    "- heavy use of **alignment**, **MultiIndex**, **time series**, and **missing data strategies**\n",
    "- correctness checks with `assert`\n",
    "- focuses on idiomatic, vectorized Pandas patterns\n",
    "\n",
    "> Run each problem cell, then compare with the solution cell below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 — Alignment + broadcasting with `axis=0` vs raw NumPy\n",
    "\n",
    "You have a Series of weights and a Series of values with different label sets.\n",
    "\n",
    "Tasks:\n",
    "1) Compute a **weighted sum** over the intersection of labels only.\n",
    "2) Compute a weighted sum treating missing weights as 0.\n",
    "3) Show how a naive `.to_numpy()` approach can silently give a different answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "values  = pd.Series({'a': 10, 'b': 20, 'c': 30, 'd': 40})\n",
    "weights = pd.Series({'b': 0.2, 'c': 0.3, 'e': 0.9})\n",
    "\n",
    "# YOUR WORK:\n",
    "# inter_weighted_sum = ...\n",
    "# weighted_sum_missing0 = ...\n",
    "# naive = ...\n",
    "# inter_weighted_sum, weighted_sum_missing0, naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(13.0), np.float64(13.0), np.float64(35.0))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "# 1) intersection only\n",
    "common = values.index.intersection(weights.index)\n",
    "inter_weighted_sum = (values.loc[common] * weights.loc[common]).sum()\n",
    "\n",
    "# 2) missing weights treated as 0\n",
    "aligned_w = weights.reindex(values.index, fill_value=0.0)\n",
    "weighted_sum_missing0 = (values * aligned_w).sum()\n",
    "\n",
    "# 3) naive numpy ignores labels and can mismatch ordering/coverage\n",
    "naive = (values.to_numpy()[: len(weights.to_numpy())] * weights.to_numpy()).sum()\n",
    "\n",
    "inter_weighted_sum, weighted_sum_missing0, naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 — Time series: resample + fill strategy + label-safe slicing\n",
    "\n",
    "You have irregular timestamped sensor readings.\n",
    "\n",
    "Tasks:\n",
    "1) Convert to a **minute frequency** Series using `resample('1min')`.\n",
    "2) Fill missing minutes using **time interpolation**.\n",
    "3) Compute a 5-minute rolling **median**.\n",
    "4) Slice a time window using `.loc[start:end]` (label-inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime([\n",
    "    '2025-03-01 12:00:00',\n",
    "    '2025-03-01 12:02:00',\n",
    "    '2025-03-01 12:07:00',\n",
    "    '2025-03-01 12:08:00'\n",
    "])\n",
    "x = pd.Series([0.0, 1.0, 0.5, 0.8], index=t, name='sensor')\n",
    "\n",
    "# YOUR WORK:\n",
    "# per_min = ...\n",
    "# filled = ...\n",
    "# roll_med = ...\n",
    "# window = ...\n",
    "# per_min.head(10), filled.head(10), roll_med.head(10), window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025-03-01 12:00:00    0.0\n",
       " 2025-03-01 12:01:00    NaN\n",
       " 2025-03-01 12:02:00    1.0\n",
       " 2025-03-01 12:03:00    NaN\n",
       " 2025-03-01 12:04:00    NaN\n",
       " 2025-03-01 12:05:00    NaN\n",
       " 2025-03-01 12:06:00    NaN\n",
       " 2025-03-01 12:07:00    0.5\n",
       " 2025-03-01 12:08:00    0.8\n",
       " Freq: min, Name: sensor, dtype: float64,\n",
       " 2025-03-01 12:00:00    0.0\n",
       " 2025-03-01 12:01:00    0.5\n",
       " 2025-03-01 12:02:00    1.0\n",
       " 2025-03-01 12:03:00    0.9\n",
       " 2025-03-01 12:04:00    0.8\n",
       " 2025-03-01 12:05:00    0.7\n",
       " 2025-03-01 12:06:00    0.6\n",
       " 2025-03-01 12:07:00    0.5\n",
       " 2025-03-01 12:08:00    0.8\n",
       " Freq: min, Name: sensor, dtype: float64,\n",
       " 2025-03-01 12:00:00    0.00\n",
       " 2025-03-01 12:01:00    0.25\n",
       " 2025-03-01 12:02:00    0.50\n",
       " 2025-03-01 12:03:00    0.70\n",
       " 2025-03-01 12:04:00    0.80\n",
       " 2025-03-01 12:05:00    0.80\n",
       " 2025-03-01 12:06:00    0.80\n",
       " 2025-03-01 12:07:00    0.70\n",
       " 2025-03-01 12:08:00    0.70\n",
       " Freq: min, Name: sensor, dtype: float64,\n",
       " 2025-03-01 12:01:00    0.5\n",
       " 2025-03-01 12:02:00    1.0\n",
       " 2025-03-01 12:03:00    0.9\n",
       " 2025-03-01 12:04:00    0.8\n",
       " 2025-03-01 12:05:00    0.7\n",
       " 2025-03-01 12:06:00    0.6\n",
       " Freq: min, Name: sensor, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "per_min = x.resample('1min').mean()  # creates NaNs for missing minutes\n",
    "filled = per_min.interpolate(method='time')\n",
    "roll_med = filled.rolling('5min').median()  # time-based window\n",
    "window = filled.loc['2025-03-01 12:01:00':'2025-03-01 12:06:00']\n",
    "\n",
    "per_min.head(10), filled.head(10), roll_med.head(10), window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 — `asof` semantics: last valid observation before each timestamp\n",
    "\n",
    "Task:\n",
    "- Given a minute-level series with missing values, compute the value **as-of** certain query times.\n",
    "- Use `asof` and confirm results match a manual approach.\n",
    "\n",
    "Note: `asof` returns the last non-NA value up to the label (requires sorted index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range('2025-04-01 09:00', periods=6, freq='min')\n",
    "s = pd.Series([np.nan, 1.0, np.nan, 2.0, np.nan, 3.0], index=idx, name='v')\n",
    "queries = pd.to_datetime(['2025-04-01 09:00', '2025-04-01 09:02', '2025-04-01 09:05'])\n",
    "\n",
    "# YOUR WORK:\n",
    "# asof_vals = ...\n",
    "# manual = ...\n",
    "# asof_vals, manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2025-04-01 09:00:00    NaN\n",
       " 2025-04-01 09:02:00    1.0\n",
       " 2025-04-01 09:05:00    3.0\n",
       " Name: asof, dtype: float64,\n",
       " 2025-04-01 09:00:00    NaN\n",
       " 2025-04-01 09:02:00    1.0\n",
       " 2025-04-01 09:05:00    3.0\n",
       " Name: manual, dtype: float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "s_sorted = s.sort_index()\n",
    "asof_vals = pd.Series([s_sorted.asof(t) for t in queries], index=queries, name='asof')\n",
    "\n",
    "def manual_asof(series, t):\n",
    "    sub = series.loc[:t].dropna()\n",
    "    return sub.iloc[-1] if len(sub) else np.nan\n",
    "\n",
    "manual = pd.Series([manual_asof(s_sorted, t) for t in queries], index=queries, name='manual')\n",
    "\n",
    "assert asof_vals.equals(manual)\n",
    "asof_vals, manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 — Duplicate index: stable disambiguation with (label, occurrence)\n",
    "\n",
    "You have duplicates and need to refer to *\"the 3rd occurrence of a label\"* reliably.\n",
    "\n",
    "Tasks:\n",
    "1) Build a **MultiIndex** `(label, occurrence)` where occurrence counts within each label.\n",
    "2) Use it to update only `('city', 2)` (3rd city) to `'Paris'`.\n",
    "3) Return back to a plain index (drop the occurrence level) while keeping values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.Series(\n",
    "    ['USA', 'Topeka', 'France', 'Lyon', 'UK', 'Glasgow', 'Spain', 'Madrid'],\n",
    "    index=['country', 'city', 'country', 'city', 'country', 'city', 'country', 'city'],\n",
    "    name='Areas'\n",
    ")\n",
    "\n",
    "# YOUR WORK:\n",
    "# occ = ...\n",
    "# mi = ...\n",
    "# updated = ...\n",
    "# back = ...\n",
    "# mi, updated, back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label    occ\n",
       " country  0          USA\n",
       " city     0       Topeka\n",
       " country  1       France\n",
       " city     1         Lyon\n",
       " country  2           UK\n",
       " city     2      Glasgow\n",
       " country  3        Spain\n",
       " city     3       Madrid\n",
       " Name: Areas, dtype: object,\n",
       " label    occ\n",
       " country  0         USA\n",
       " city     0      Topeka\n",
       " country  1      France\n",
       " city     1        Lyon\n",
       " country  2          UK\n",
       " city     2       Paris\n",
       " country  3       Spain\n",
       " city     3      Madrid\n",
       " Name: Areas, dtype: object,\n",
       " label\n",
       " country       USA\n",
       " city       Topeka\n",
       " country    France\n",
       " city         Lyon\n",
       " country        UK\n",
       " city        Paris\n",
       " country     Spain\n",
       " city       Madrid\n",
       " Name: Areas, dtype: object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "occ = base.groupby(level=0).cumcount()\n",
    "mi = base.copy()\n",
    "mi.index = pd.MultiIndex.from_arrays([base.index, occ], names=['label', 'occ'])\n",
    "\n",
    "updated = mi.copy()\n",
    "updated.loc[('city', 2)] = 'Paris'\n",
    "\n",
    "back = updated.copy()\n",
    "back.index = back.index.get_level_values('label')\n",
    "\n",
    "mi, updated, back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 — MultiIndex time series: per-group resample then align back\n",
    "\n",
    "You have sales for multiple stores with irregular timestamps.\n",
    "\n",
    "Tasks:\n",
    "1) Build a MultiIndex Series `(store, ts)`.\n",
    "2) For each store, resample to 5-minute frequency and forward-fill.\n",
    "3) Return a **single** Series with MultiIndex `(store, ts)`.\n",
    "\n",
    "Hint: `groupby(level=0).apply(...)` then sort the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = ['S1', 'S1', 'S1', 'S2', 'S2']\n",
    "ts = pd.to_datetime([\n",
    "    '2025-05-01 10:00', '2025-05-01 10:07', '2025-05-01 10:20',\n",
    "    '2025-05-01 10:02', '2025-05-01 10:19'\n",
    "])\n",
    "y = pd.Series([10, 12, 15, 7, 11], name='sales')\n",
    "sales = pd.Series(y.to_numpy(), index=pd.MultiIndex.from_arrays([stores, ts], names=['store', 'ts']))\n",
    "\n",
    "# YOUR WORK:\n",
    "# res = ...\n",
    "# res.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store  ts                 \n",
       "S1     2025-05-01 10:00:00    10.0\n",
       "       2025-05-01 10:05:00    12.0\n",
       "       2025-05-01 10:10:00    12.0\n",
       "       2025-05-01 10:15:00    12.0\n",
       "       2025-05-01 10:20:00    15.0\n",
       "S2     2025-05-01 10:00:00     7.0\n",
       "       2025-05-01 10:05:00     7.0\n",
       "       2025-05-01 10:10:00     7.0\n",
       "       2025-05-01 10:15:00    11.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "def resample_ffill(s_store: pd.Series) -> pd.Series:\n",
    "    # s_store index is 'ts' after groupby(level=0) drops the grouped level in the view\n",
    "    s_store = s_store.droplevel('store').sort_index()\n",
    "    out = s_store.resample('5min').mean().ffill()\n",
    "    return out\n",
    "\n",
    "res = sales.groupby(level='store').apply(resample_ffill)\n",
    "res.index = res.index.set_names(['store', 'ts'])\n",
    "res = res.sort_index()\n",
    "\n",
    "res.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 — `where` vs `mask` + preserving dtype\n",
    "\n",
    "Task:\n",
    "- Replace negative values with 0, but keep dtype and index.\n",
    "- Then replace values above a threshold with `NaN`.\n",
    "\n",
    "Requirements:\n",
    "- Use `where`/`mask` (not `apply`).\n",
    "- Demonstrate the dtype changes and explain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.Series([5, -2, 3, -1, 10], index=list('abcde'))\n",
    "\n",
    "# YOUR WORK:\n",
    "# nonneg = ...\n",
    "# capped_nan = ...\n",
    "# nonneg, capped_nan, nonneg.dtype, capped_nan.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(a     5\n",
       " b     0\n",
       " c     3\n",
       " d     0\n",
       " e    10\n",
       " dtype: int64,\n",
       " a    5.0\n",
       " b    0.0\n",
       " c    3.0\n",
       " d    0.0\n",
       " e    NaN\n",
       " dtype: float64,\n",
       " dtype('int64'),\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "nonneg = z.mask(z < 0, 0)  # mask: replace where condition True\n",
    "capped_nan = nonneg.where(nonneg <= 6)  # where: keep where condition True, else NaN\n",
    "\n",
    "# Note: introducing NaN forces float dtype for numeric series (unless nullable dtype is used)\n",
    "nonneg, capped_nan, nonneg.dtype, capped_nan.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 — Nullable dtypes: keep integers while allowing missing\n",
    "\n",
    "Task:\n",
    "- Convert an integer Series to a **nullable integer dtype**.\n",
    "- Insert missing values without converting to float.\n",
    "- Compare with the standard NumPy int behavior.\n",
    "\n",
    "Goal: end with dtype `Int64` (capital I), not `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.Series([1, 2, 3], index=list('xyz'))\n",
    "\n",
    "# YOUR WORK:\n",
    "# q_nullable = ...\n",
    "# q_nullable.loc['y'] = ...\n",
    "# q_float = ...\n",
    "# q_nullable, q_nullable.dtype, q_float, q_float.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(x       1\n",
       " y    <NA>\n",
       " z       3\n",
       " dtype: Int64,\n",
       " Int64Dtype(),\n",
       " x    1.0\n",
       " y    NaN\n",
       " z    3.0\n",
       " dtype: float64,\n",
       " dtype('float64'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "q_nullable = q.astype('Int64')\n",
    "q_nullable.loc['y'] = pd.NA\n",
    "\n",
    "# Standard behavior: inserting np.nan into int series upcasts to float\n",
    "q_float = q.copy()\n",
    "q_float.loc['y'] = np.nan\n",
    "\n",
    "assert str(q_nullable.dtype) == 'Int64'\n",
    "q_nullable, q_nullable.dtype, q_float, q_float.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8 — `explode` + `value_counts` pipeline with missing handling\n",
    "\n",
    "You have a Series of comma-separated tags, some missing.\n",
    "\n",
    "Tasks:\n",
    "1) Split into lists, explode into one tag per row.\n",
    "2) Strip whitespace and drop empties.\n",
    "3) Compute tag frequencies.\n",
    "\n",
    "Constraints:\n",
    "- Do not use Python loops.\n",
    "- Treat missing as having no tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pd.Series(['a, b, c', None, 'b, c', 'a,  ', 'c'], index=[101, 102, 103, 104, 105])\n",
    "\n",
    "# YOUR WORK:\n",
    "# freq = ...\n",
    "# freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c    3\n",
       "a    2\n",
       "b    2\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "freq = (\n",
    "    tags.fillna('')\n",
    "        .str.split(',')\n",
    "        .explode()\n",
    "        .astype('string')\n",
    "        .str.strip()\n",
    "        .loc[lambda s: s.ne('')]\n",
    "        .value_counts()\n",
    ")\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9 — Stable ranking with ties, then select “top-k per group”\n",
    "\n",
    "You have a MultiIndex Series `(group, item)` of scores.\n",
    "\n",
    "Tasks:\n",
    "1) Compute a **dense rank** within each group (1 = best score).\n",
    "2) Select all items with rank <= 2.\n",
    "3) Return results sorted by `(group, rank, score desc)`.\n",
    "\n",
    "Note: Do not use Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.MultiIndex.from_tuples(\n",
    "    [('G1','a'), ('G1','b'), ('G1','c'), ('G2','a'), ('G2','b'), ('G2','c')],\n",
    "    names=['group', 'item']\n",
    ")\n",
    "scores = pd.Series([90, 90, 80, 70, 85, 85], index=idx, name='score')\n",
    "\n",
    "# YOUR WORK:\n",
    "# rank = ...\n",
    "# top = ...\n",
    "# top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group  item  rank\n",
       "G1     a     1       90\n",
       "       b     1       90\n",
       "       c     2       80\n",
       "G2     b     1       85\n",
       "       c     1       85\n",
       "       a     2       70\n",
       "Name: score, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "rank = scores.groupby(level='group').rank(method='dense', ascending=False).astype('int64')\n",
    "top = scores.loc[rank <= 2]\n",
    "\n",
    "# Sort by (group, rank asc, score desc)\n",
    "tmp = pd.DataFrame({'score': top, 'rank': rank.loc[top.index]})\n",
    "tmp = tmp.sort_values(['group', 'rank', 'score'], ascending=[True, True, False])\n",
    "\n",
    "# Return as a Series with a helpful MultiIndex including rank\n",
    "out = tmp.set_index('rank', append=True)['score']\n",
    "out.index = out.index.set_names(['group', 'item', 'rank'])\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10 — `searchsorted` on a sorted index: fast interval lookup\n",
    "\n",
    "You have a sorted Series of thresholds (index is numeric) mapping to categories.\n",
    "Given query values, assign each query to the category whose threshold is the **largest <= query**.\n",
    "\n",
    "Tasks:\n",
    "1) Use `Index.searchsorted` (vectorized) to assign categories.\n",
    "2) Handle queries below the smallest threshold as `None`.\n",
    "3) Return a Series aligned to the query index.\n",
    "\n",
    "This is a common expert pattern for fast binning without `cut`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = pd.Series(['low', 'med', 'high'], index=pd.Index([0, 50, 100], name='min_score'))\n",
    "queries = pd.Series([10, 55, 120, -5], index=list('wxyz'), name='score')\n",
    "\n",
    "# YOUR WORK:\n",
    "# assigned = ...\n",
    "# assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w     low\n",
       "x     med\n",
       "y    high\n",
       "z    None\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION\n",
    "idx = thresholds.index\n",
    "pos = idx.searchsorted(queries.to_numpy(), side='right') - 1\n",
    "\n",
    "assigned = pd.Series(index=queries.index, dtype='object', name='category')\n",
    "valid = pos >= 0\n",
    "assigned.loc[valid] = thresholds.iloc[pos[valid]].to_numpy()\n",
    "assigned.loc[~valid] = None\n",
    "\n",
    "assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
