{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Examples Reading CSV Files — Advanced Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll practice more advanced (but still very practical) patterns for working with CSV files using Python's built-in [`csv`](https://docs.python.org/3/library/csv.html) module.\n",
    "\n",
    "We will assume that the following files are available in the current working directory:\n",
    "\n",
    "- `nasdaq.csv`\n",
    "- `st-2001est-01.csv` (US Census data)\n",
    "\n",
    "Each exercise is followed immediately by a reference solution written with common best practices (context managers, `newline=''` when opening CSVs, small reusable functions, and robust type conversion).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "from collections import namedtuple\n",
    "from io import StringIO\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 — Parsing `nasdaq.csv` into dictionaries\n",
    "\n",
    "The earlier example parsed `nasdaq.csv` into a list of lists and manually converted the last column to a `float`.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Use `csv.DictReader` to parse `nasdaq.csv` into a list of dictionaries, where each dictionary represents one row.\n",
    "2. Convert the `last_sale` column (or the last numeric column in the file) from a string to `float`.\n",
    "3. Return the parsed data from a function called `parse_nasdaq_dict`.\n",
    "\n",
    "Your function should:\n",
    "\n",
    "- Accept a filename.\n",
    "- Not modify the header names (use them as-is from the CSV file).\n",
    "- Leave non-numeric fields as strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Symbol': 'AAIT',\n",
       "  'Company Name': 'iShares MSCI All Country Asia Information Technology Index Fund',\n",
       "  'Security Name': 'iShares MSCI All Country Asia Information Technology Index Fund',\n",
       "  'Market Category': 'G',\n",
       "  'Test Issue': 'N',\n",
       "  'Financial Status': 'N',\n",
       "  'Round Lot Size': 100.0},\n",
       " {'Symbol': 'AAL',\n",
       "  'Company Name': 'American Airlines Group, Inc.',\n",
       "  'Security Name': 'American Airlines Group, Inc. - Common Stock',\n",
       "  'Market Category': 'Q',\n",
       "  'Test Issue': 'N',\n",
       "  'Financial Status': 'N',\n",
       "  'Round Lot Size': 100.0},\n",
       " {'Symbol': 'AAME',\n",
       "  'Company Name': 'Atlantic American Corporation',\n",
       "  'Security Name': 'Atlantic American Corporation - Common Stock',\n",
       "  'Market Category': 'G',\n",
       "  'Test Issue': 'N',\n",
       "  'Financial Status': 'N',\n",
       "  'Round Lot Size': 100.0},\n",
       " {'Symbol': 'AAOI',\n",
       "  'Company Name': 'Applied Optoelectronics, Inc.',\n",
       "  'Security Name': 'Applied Optoelectronics, Inc. - Common Stock',\n",
       "  'Market Category': 'G',\n",
       "  'Test Issue': 'N',\n",
       "  'Financial Status': 'N',\n",
       "  'Round Lot Size': 100.0},\n",
       " {'Symbol': 'AAON',\n",
       "  'Company Name': 'AAON, Inc.',\n",
       "  'Security Name': 'AAON, Inc. - Common Stock',\n",
       "  'Market Category': 'Q',\n",
       "  'Test Issue': 'N',\n",
       "  'Financial Status': 'N',\n",
       "  'Round Lot Size': 100.0}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_nasdaq_dict(file_name):\n",
    "    \"\"\"Parse nasdaq-style CSV into a list of dictionaries.\n",
    "\n",
    "    The function assumes the CSV has a header row and that the last\n",
    "    column contains numeric values that should be converted to float.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    # newline='' is recommended by the csv module docs\n",
    "    with open(file_name, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "        if not fieldnames:\n",
    "            return records  # empty file\n",
    "\n",
    "        last_field = fieldnames[-1]\n",
    "\n",
    "        for row in reader:\n",
    "            # Make a shallow copy before mutating (good practice when in doubt)\n",
    "            row = dict(row)\n",
    "            # Convert last column to float if the value is not empty\n",
    "            value = row.get(last_field, '').strip()\n",
    "            if value:\n",
    "                row[last_field] = float(value)\n",
    "            else:\n",
    "                row[last_field] = None\n",
    "            records.append(row)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "# Example usage (will print first 5 parsed rows if the file exists)\n",
    "nasdaq_file = 'nasdaq.csv'\n",
    "nasdaq_records = parse_nasdaq_dict(nasdaq_file)\n",
    "nasdaq_records[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 — Streaming data and computing statistics\n",
    "\n",
    "So far we have loaded all the CSV data into memory at once. For large files, it is more efficient to *stream* rows.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Write a generator function `iter_nasdaq_last_prices(file_name)` that:\n",
    "   - Uses `csv.reader` to iterate over `nasdaq.csv`.\n",
    "   - Skips the header.\n",
    "   - Yields the last column converted to `float` for each row.\n",
    "2. Use that generator to compute the average of the last prices without ever storing the full list of prices in memory at once.\n",
    "\n",
    "*Hint:* A generator is just a function that uses `yield` instead of `return`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.96966632962588"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iter_nasdaq_last_prices(file_name):\n",
    "    \"\"\"Yield last prices from a nasdaq-style CSV, one at a time.\n",
    "\n",
    "    Assumes the last column is numeric and contains the last sale price.\n",
    "    \"\"\"\n",
    "    with open(file_name, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        # Skip header row\n",
    "        headers = next(reader, None)\n",
    "        if headers is None:\n",
    "            return  # empty file\n",
    "\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue  # skip empty lines\n",
    "            try:\n",
    "                yield float(row[-1])\n",
    "            except ValueError:\n",
    "                # If conversion fails, skip that row (or log, depending on your needs)\n",
    "                continue\n",
    "\n",
    "\n",
    "def average_last_price(file_name):\n",
    "    prices = list(iter_nasdaq_last_prices(file_name))\n",
    "    if not prices:\n",
    "        return None\n",
    "    return sum(prices) / len(prices)\n",
    "\n",
    "\n",
    "# Example usage (if the file exists)\n",
    "avg_price = average_last_price(nasdaq_file)\n",
    "avg_price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In a *truly* memory-conscious implementation we would avoid converting the generator to a list in `average_last_price` and instead compute the running sum and count. Here we keep the code simple and focus on the generator pattern itself. See the next small variation for a streaming-only implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.96966632962588"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_last_price_streaming(file_name):\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for price in iter_nasdaq_last_prices(file_name):\n",
    "        total += price\n",
    "        count += 1\n",
    "    return (total / count) if count else None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "average_last_price_streaming(nasdaq_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 — Parsing Census data into `namedtuple`s\n",
    "\n",
    "Previously, we parsed `st-2001est-01.csv` into lists, cleaning up the thousands separators and converting strings to `int`.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Define a `namedtuple` type called `CensusRow` with fields that match the header row of the census CSV file.\n",
    "2. Write a function `load_census_records(file_name)` that:\n",
    "   - Uses `csv.reader` to read the file.\n",
    "   - Uses the first row as headers.\n",
    "   - Cleans any numeric columns by removing commas and converting them to `int`.\n",
    "   - Returns a list of `CensusRow` objects.\n",
    "3. Print the first three records to verify that parsing works correctly.\n",
    "\n",
    "You may assume that **all columns except the first** are numeric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CensusRow(Geographic_Area='United States', July_1_2001_Estimate=284796887, July_1_2000_Estimate=282124631, April_1_2000_Population_Estimates_Base=281421906),\n",
       " CensusRow(Geographic_Area='Alabama', July_1_2001_Estimate=4464356, July_1_2000_Estimate=4451493, April_1_2000_Population_Estimates_Base=4447100),\n",
       " CensusRow(Geographic_Area='Alaska', July_1_2001_Estimate=634892, July_1_2000_Estimate=627601, April_1_2000_Population_Estimates_Base=626932)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_census_records(file_name):\n",
    "    \"\"\"Load census CSV data into a list of CensusRow namedtuples.\n",
    "\n",
    "    Assumes column 0 is a text label and all remaining columns are integers\n",
    "    possibly using commas as thousands separators.\n",
    "    \"\"\"\n",
    "    with open(file_name, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader, None)\n",
    "        if headers is None:\n",
    "            return []\n",
    "\n",
    "        # Normalize headers to valid Python identifiers\n",
    "        def normalize_header(h):\n",
    "            name = h.strip()\n",
    "            # Replace spaces with underscores\n",
    "            name = name.replace(' ', '_')\n",
    "            # Remove commas\n",
    "            name = name.replace(',', '')\n",
    "            # Remove any remaining non-alphanumeric/underscore chars\n",
    "            name = re.sub(r'\\W', '', name)\n",
    "            # If it starts with a digit, prefix with 'f_'\n",
    "            if name and name[0].isdigit():\n",
    "                name = 'f_' + name\n",
    "            # Fallback if the header was empty or all invalid chars\n",
    "            if not name:\n",
    "                name = 'field'\n",
    "            return name\n",
    "\n",
    "        normalized = [normalize_header(h) for h in headers]\n",
    "\n",
    "        # Ensure field names are unique\n",
    "        seen = {}\n",
    "        final_headers = []\n",
    "        for name in normalized:\n",
    "            base = name\n",
    "            if base not in seen:\n",
    "                seen[base] = 0\n",
    "                final_headers.append(base)\n",
    "            else:\n",
    "                seen[base] += 1\n",
    "                final_headers.append(f\"{base}_{seen[base]}\")\n",
    "\n",
    "        CensusRow = namedtuple('CensusRow', final_headers)\n",
    "\n",
    "        records = []\n",
    "        for row in reader:\n",
    "            if not row:\n",
    "                continue\n",
    "\n",
    "            area = row[0].strip()\n",
    "            numeric_fields = []\n",
    "            for field in row[1:]:\n",
    "                cleaned = field.replace(',', '').strip()\n",
    "                numeric_fields.append(int(cleaned))\n",
    "\n",
    "            record = CensusRow(area, *numeric_fields)\n",
    "            records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "\n",
    "census_file = 'st-2001est-01.csv'\n",
    "census_records = load_census_records(census_file)\n",
    "census_records[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is in `namedtuple` form, working with it becomes more expressive and self-documenting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: find the area with the largest value in the last numeric column\n",
    "if census_records:\n",
    "    last_field_name = census_records[0]._fields[-1]\n",
    "    max_row = max(census_records, key=lambda r: getattr(r, last_field_name))\n",
    "    last_value = getattr(max_row, last_field_name)\n",
    "    (max_row, last_field_name, last_value)\n",
    "else:\n",
    "    None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 — Detecting dialects with `csv.Sniffer`\n",
    "\n",
    "Sometimes you receive a CSV file with an unknown delimiter and quoting rules. Python's `csv.Sniffer` can often detect these automatically.\n",
    "\n",
    "We'll simulate such a file using an in-memory text buffer.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Create a multi-line string representing CSV data where:\n",
    "   - Fields are separated by semicolons (`;`).\n",
    "   - Fields containing spaces are quoted with double quotes.\n",
    "2. Use `csv.Sniffer().sniff` to detect the dialect from a sample of this string.\n",
    "3. Parse the data using the detected dialect.\n",
    "4. Convert the last column to `int`.\n",
    "\n",
    "Implement this in a function `parse_with_sniffer(text)` that returns a list of rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['city', 'state', 'population'],\n",
       " ['New York', 'NY', 8419600],\n",
       " ['Los Angeles', 'CA', 3980400],\n",
       " ['Chicago', 'IL', 2716000]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"\"\"city;state;population\n",
    "\"New York\";NY;8419600\n",
    "\"Los Angeles\";CA;3980400\n",
    "\"Chicago\";IL;2716000\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_with_sniffer(text):\n",
    "    \"\"\"Parse CSV text using csv.Sniffer to detect the dialect.\n",
    "\n",
    "    Returns a list of rows where the last column has been converted to int.\n",
    "    \"\"\"\n",
    "    # Use a StringIO buffer so the csv module can read from it like a file\n",
    "    buffer = StringIO(text)\n",
    "    sample = buffer.read(1024)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    sniffer = csv.Sniffer()\n",
    "    dialect = sniffer.sniff(sample)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    reader = csv.reader(buffer, dialect=dialect)\n",
    "    rows = []\n",
    "    headers = next(reader, None)\n",
    "    if headers is not None:\n",
    "        rows.append(headers)\n",
    "\n",
    "    for row in reader:\n",
    "        if not row:\n",
    "            continue\n",
    "        # Convert last field to int\n",
    "        row[-1] = int(row[-1])\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "parsed_rows = parse_with_sniffer(sample_text)\n",
    "parsed_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the detected dialect to see what `Sniffer` discovered:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(';', '\"', False, False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer = StringIO(sample_text)\n",
    "sample = buffer.read(1024)\n",
    "buffer.seek(0)\n",
    "dialect = csv.Sniffer().sniff(sample)\n",
    "dialect.delimiter, dialect.quotechar, dialect.doublequote, dialect.skipinitialspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 — Handling malformed rows gracefully\n",
    "\n",
    "Real-world CSV files often contain malformed rows (missing columns, bad numeric values, etc.). Your parsing code should handle these cases without crashing.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Write a function `safe_parse_census(file_name)` that:\n",
    "   - Uses `csv.reader` to read the census CSV.\n",
    "   - Uses the first row as headers (like before).\n",
    "   - Tries to parse numeric columns by removing commas and converting to `int`.\n",
    "   - If a row fails numeric conversion, prints a *warning* including the row number and skips that row.\n",
    "2. Return the successfully parsed rows as a list of lists: `[header_row, row1, row2, ...]`.\n",
    "\n",
    "This exercise focuses on robust error handling rather than data structures like `namedtuple`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Geographic Area',\n",
       "  'July 1, 2001 Estimate',\n",
       "  'July 1, 2000 Estimate',\n",
       "  'April 1, 2000 Population Estimates Base'],\n",
       " ['United States', 284796887, 282124631, 281421906],\n",
       " ['Alabama', 4464356, 4451493, 4447100],\n",
       " ['Alaska', 634892, 627601, 626932],\n",
       " ['Arizona', 5307331, 5165274, 5130632]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def safe_parse_census(file_name):\n",
    "    \"\"\"Parse census CSV file and skip malformed rows.\n",
    "\n",
    "    Returns a list of rows, with the first element being the header row.\n",
    "    Prints a warning message to stdout for any row that cannot be parsed.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    with open(file_name, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader, None)\n",
    "        if headers is None:\n",
    "            return results\n",
    "\n",
    "        results.append(headers)\n",
    "\n",
    "        for row_number, row in enumerate(reader, start=2):  # header is row 1\n",
    "            if not row:\n",
    "                continue\n",
    "            try:\n",
    "                area = row[0]\n",
    "                numeric = [int(field.replace(',', '').strip()) for field in row[1:]]\n",
    "                parsed_row = [area] + numeric\n",
    "                results.append(parsed_row)\n",
    "            except (ValueError, IndexError) as ex:\n",
    "                print(f\"Warning: skipping malformed row {row_number}: {row!r} ({ex})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "safe_census_data = safe_parse_census(census_file)\n",
    "# Show the first few parsed data rows (excluding header)\n",
    "safe_census_data[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
