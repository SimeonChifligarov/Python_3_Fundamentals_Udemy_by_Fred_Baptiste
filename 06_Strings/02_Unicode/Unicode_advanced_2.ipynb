{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode — Advanced Exercises (No Solutions)\n",
    "\n",
    "These problems deepen your understanding of Unicode in Python. You may use only the standard library (e.g. `unicodedata`, `codecs`, `locale`, `re`, `sys`).\n",
    "\n",
    "**Tip:** Prefer `str.casefold()` to `lower()` for Unicode-aware case-insensitive logic, and use `unicodedata.normalize()` for normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 — Show-Code-Points Utility\n",
    "Write a function `show_codepoints(s)` that returns a list of tuples `(char, codepoint_hex, name, category)` for each character in `s`.\n",
    "\n",
    "- `codepoint_hex` should look like `U+1F600` (always 4 or 6+ hex digits, uppercase).\n",
    "- Use `unicodedata.name(c, '<no name>')` and `unicodedata.category(c)`.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "show_codepoints('Aα😀')\n",
    "# [('A', 'U+0041', 'LATIN CAPITAL LETTER A', 'Lu'), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'U+0041', 'LATIN CAPITAL LETTER A', 'Lu'),\n",
       " ('α', 'U+03B1', 'GREEK SMALL LETTER ALPHA', 'Ll'),\n",
       " ('😀', 'U+01F600', 'GRINNING FACE', 'So')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata as _ud\n",
    "\n",
    "def show_codepoints(s: str):\n",
    "    \"\"\"Return [(char, 'U+XXXX', NAME, CATEGORY), ...] for s.\"\"\"\n",
    "    result = []\n",
    "    for ch in s:\n",
    "        cp = f\"U+{ord(ch):04X}\" if ord(ch) <= 0xFFFF else f\"U+{ord(ch):06X}\"\n",
    "        name = _ud.name(ch, '<no name>')\n",
    "        cat = _ud.category(ch)\n",
    "        result.append((ch, cp, name, cat))\n",
    "    return result\n",
    "\n",
    "# TODO: test\n",
    "show_codepoints('Aα😀')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 — Canonical Equivalence Check (NFC vs NFD)\n",
    "Implement `canonically_equal(a, b)` that returns `True` if strings `a` and `b` are canonically equivalent (equal after NFC normalization). Demonstrate on `'é'` (precomposed) and `'e\\u0301'` (combining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def canonically_equal(a: str, b: str) -> bool:\n",
    "    # TODO: implement using unicodedata.normalize\n",
    "    return _ud.normalize('NFC', a) == _ud.normalize('NFC', b)\n",
    "\n",
    "# TODO: demo\n",
    "s1 = 'é'\n",
    "s2 = 'e\\u0301'\n",
    "canonically_equal(s1, s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — Strip Diacritics (Accent Folding)\n",
    "Write `strip_diacritics(s)` that removes combining marks while keeping base characters.\n",
    "\n",
    "Hints:\n",
    "- Normalize to NFD, drop code points with `unicodedata.combining(c) != 0`, then re-normalize to NFC.\n",
    "\n",
    "Example: `strip_diacritics('Café déjà vu') -> 'Cafe deja vu'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cafe deja vu — naive facade'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strip_diacritics(s: str) -> str:\n",
    "    nfd = _ud.normalize('NFD', s)\n",
    "    filtered = ''.join(ch for ch in nfd if _ud.combining(ch) == 0)\n",
    "    return _ud.normalize('NFC', filtered)\n",
    "\n",
    "# TODO: demo\n",
    "strip_diacritics('Café déjà vu — naïve façade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — Unicode-Aware Palindrome\n",
    "Implement `is_unicode_palindrome(s)` that returns `True` for palindromes when compared in a **case-insensitive** and **diacritics-insensitive** manner.\n",
    "\n",
    "Process:\n",
    "1. Normalize (NFD), strip combining marks, normalize back (NFC).\n",
    "2. Apply `casefold()`.\n",
    "3. Remove all characters with category starting with `Z` (separators) and punctuation categories (`P*`).\n",
    "4. Compare with reversed version.\n",
    "\n",
    "Test with `'Åna'` vs `'ana'`, `'réifier'` (not palindrome), `'Noël, Léon!'` (is palindrome under this definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Åna': True, 'ana': True, 'réifier': True, 'Noël, Léon!': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re as _re\n",
    "\n",
    "def _normalize_for_palindrome(s: str) -> str:\n",
    "    s = _ud.normalize('NFD', s)\n",
    "    s = ''.join(ch for ch in s if _ud.combining(ch) == 0)\n",
    "    s = _ud.normalize('NFC', s).casefold()\n",
    "    # remove separators (Z*) and punctuation (P*) via category prefixes\n",
    "    return ''.join(ch for ch in s if not (_ud.category(ch).startswith('Z') or _ud.category(ch).startswith('P')))\n",
    "\n",
    "def is_unicode_palindrome(s: str) -> bool:\n",
    "    t = _normalize_for_palindrome(s)\n",
    "    return t == t[::-1]\n",
    "\n",
    "# TODO: tests\n",
    "tests = [\n",
    "    'Åna',\n",
    "    'ana',\n",
    "    'réifier',\n",
    "    'Noël, Léon!',\n",
    "]\n",
    "{x: is_unicode_palindrome(x) for x in tests}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 — Case-Insensitive, Diacritics-Insensitive Set\n",
    "Implement `unique_words(text)` that returns unique words ignoring case and diacritics, preserving first-seen original spelling.\n",
    "\n",
    "Steps:\n",
    "- Tokenize by splitting on any non-letter (`unicodedata.category(ch)` not starting with `L`).\n",
    "- For each token, compute a canonical key via `strip_diacritics(token).casefold()`.\n",
    "- Return a dict `{canonical_key: first_original_form}` or a list of originals in first-seen order.\n",
    "\n",
    "Test on: `\"Café cafe CAFÉ caffè déjà Deja déja\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Café', 'caffè', 'déjà'], {'cafe': 'Café', 'caffe': 'caffè', 'deja': 'déjà'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _is_letter(ch: str) -> bool:\n",
    "    return _ud.category(ch).startswith('L')\n",
    "\n",
    "def unique_words(text: str):\n",
    "    words = []\n",
    "    buf = []\n",
    "    for ch in text:\n",
    "        if _is_letter(ch):\n",
    "            buf.append(ch)\n",
    "        else:\n",
    "            if buf:\n",
    "                words.append(''.join(buf))\n",
    "                buf = []\n",
    "    if buf:\n",
    "        words.append(''.join(buf))\n",
    "\n",
    "    seen = {}\n",
    "    order = []\n",
    "    for w in words:\n",
    "        key = strip_diacritics(w).casefold()\n",
    "        if key not in seen:\n",
    "            seen[key] = w\n",
    "            order.append(w)\n",
    "    return order, seen\n",
    "\n",
    "# TODO: demo\n",
    "unique_words('Café cafe CAFÉ caffè déjà Deja déja')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 — Bytes Encodings Round-Trip\n",
    "Write `roundtrip_encodings(s, encodings)` that encodes `s` with each encoding and decodes back. Return a dict mapping encoding to `(ok: bool, bytes_len: int or None)`.\n",
    "\n",
    "Test on `s = 'Aα😀'` with encodings `['utf-8', 'utf-16-le', 'utf-32-be', 'latin-1']`.\n",
    "\n",
    "Note: Some encodings (e.g. `latin-1`) cannot represent all characters; mark those as `ok=False` and `bytes_len=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'utf-8': (True, 7),\n",
       " 'utf-16-le': (True, 8),\n",
       " 'utf-32-be': (True, 12),\n",
       " 'latin-1': (False, None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def roundtrip_encodings(s: str, encodings):\n",
    "    out = {}\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            b = s.encode(enc)\n",
    "            t = b.decode(enc)\n",
    "            out[enc] = (t == s, len(b))\n",
    "        except UnicodeEncodeError:\n",
    "            out[enc] = (False, None)\n",
    "    return out\n",
    "\n",
    "# TODO: demo\n",
    "roundtrip_encodings('Aα😀', ['utf-8', 'utf-16-le', 'utf-32-be', 'latin-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 — Flag Emoji → ISO Country Code\n",
    "A flag emoji like 🇧🇬 is composed of two *Regional Indicator Symbols* (RIS): U+1F1E6..U+1F1FF. Convert a flag emoji string of length 2 into an ISO-3166 alpha-2 code (`'BG'`, `'US'`, etc.).\n",
    "\n",
    "Hints:\n",
    "- RIS for `'A'` is U+1F1E6. So `chr_code - 0x1F1E6` yields 0..25, then add `ord('A')`.\n",
    "- Validate input length and range; raise `ValueError` on invalid input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BG', 'US')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flag_to_iso(flag: str) -> str:\n",
    "    if len(flag) != 2:\n",
    "        raise ValueError('Flag must be 2 code points (RIS) long')\n",
    "    base = 0x1F1E6\n",
    "    letters = []\n",
    "    for ch in flag:\n",
    "        cp = ord(ch)\n",
    "        if not (0x1F1E6 <= cp <= 0x1F1FF):\n",
    "            raise ValueError('Not a regional indicator symbol')\n",
    "        letters.append(chr(ord('A') + (cp - base)))\n",
    "    return ''.join(letters)\n",
    "\n",
    "# TODO: demo (🇧🇬 -> BG, 🇺🇸 -> US)\n",
    "flag_to_iso('🇧🇬'), flag_to_iso('🇺🇸')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 — Escape Non-ASCII\n",
    "Write `escape_non_ascii(s, by='hex')` that escapes all characters with `ord(ch) > 0x7F`.\n",
    "\n",
    "Modes:\n",
    "- `by='hex'`: use `\\uXXXX` or `\\UXXXXXXXX` (uppercase hex, zero-padded).\n",
    "- `by='name'`: use `\\N{UNICODE NAME}` where available; fallback to hex escape if name is missing.\n",
    "\n",
    "Example: `escape_non_ascii('Café 😀')` -> `'Cafe\\u00E9 \\U0001F600'` (hex mode) or `'Cafe\\N{LATIN SMALL LETTER E WITH ACUTE} \\N{GRINNING FACE}'` (name mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Caf\\\\u00E9 \\\\U0001F600',\n",
       " 'Caf\\\\N{LATIN SMALL LETTER E WITH ACUTE} \\\\N{GRINNING FACE}')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def escape_non_ascii(s: str, by='hex') -> str:\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        cp = ord(ch)\n",
    "        if cp <= 0x7F:\n",
    "            out.append(ch)\n",
    "            continue\n",
    "        if by == 'name':\n",
    "            name = _ud.name(ch, None)\n",
    "            if name:\n",
    "                out.append(f\"\\\\N{{{name}}}\")\n",
    "                continue\n",
    "            # fallthrough to hex if no name\n",
    "        # hex\n",
    "        if cp <= 0xFFFF:\n",
    "            out.append(f\"\\\\u{cp:04X}\")\n",
    "        else:\n",
    "            out.append(f\"\\\\U{cp:08X}\")\n",
    "    return ''.join(out)\n",
    "\n",
    "# TODO: demo\n",
    "escape_non_ascii('Café 😀', by='hex'), escape_non_ascii('Café 😀', by='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 — Unicode-Aware Sorting Key\n",
    "Implement `unicode_sort_key(s)` suitable for grouping user-facing strings:\n",
    "1. Normalize to NFKD.\n",
    "2. Strip diacritics (drop combining marks).\n",
    "3. Casefold.\n",
    "4. Normalize to NFC.\n",
    "\n",
    "Then sort a list like `['Éclair', 'eclair', 'élan', 'Zebra', 'Ångström', 'angstrom']` using this key and show the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ångström', 'angstrom', 'Éclair', 'eclair', 'élan', 'Zebra']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unicode_sort_key(s: str) -> str:\n",
    "    s = _ud.normalize('NFKD', s)\n",
    "    s = ''.join(ch for ch in s if _ud.combining(ch) == 0)\n",
    "    s = s.casefold()\n",
    "    return _ud.normalize('NFC', s)\n",
    "\n",
    "# TODO: demo\n",
    "items = ['Éclair', 'eclair', 'élan', 'Zebra', 'Ångström', 'angstrom']\n",
    "sorted(items, key=unicode_sort_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 — Confusables Detection (Basic)\n",
    "Implement `are_confusable(a, b)` that returns `True` if `a` and `b` become equal after applying NFKC normalization **and** casefolding.\n",
    "\n",
    "Examples to try:\n",
    "- `'K'` (Kelvin sign U+212A) vs `'K'`.\n",
    "- Full-width forms like `'ＡＢＣ'` vs `'ABC'`.\n",
    "\n",
    "> **Note**: This is a simplistic approach; real confusable detection uses Unicode confusables data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fold_nfkc(s: str) -> str:\n",
    "    return _ud.normalize('NFKC', s).casefold()\n",
    "\n",
    "def are_confusable(a: str, b: str) -> bool:\n",
    "    return fold_nfkc(a) == fold_nfkc(b)\n",
    "\n",
    "# TODO: demos\n",
    "are_confusable('K', 'K'), are_confusable('ＡＢＣ', 'ABC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11 — Grapheme Cluster Approximation\n",
    "Implement `split_graphemes_basic(s)` that *approximately* splits a string into grapheme-like clusters by grouping each base character with subsequent combining marks.\n",
    "\n",
    "Limitations: This is not full Unicode grapheme segmentation (no ZWJ, emoji sequences, Indic clusters, etc.).\n",
    "\n",
    "Test on: `'naïve café e\\u0301 🇺🇸'` (note: flag will be split into two RIS characters here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'a', 'ï', 'v', 'e', ' ', 'c', 'a', 'f', 'é', ' ', 'é', ' ', '🇺', '🇸']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_graphemes_basic(s: str):\n",
    "    clusters = []\n",
    "    buf = ''\n",
    "    for ch in s:\n",
    "        if not buf:\n",
    "            buf = ch\n",
    "            continue\n",
    "        if _ud.combining(ch):\n",
    "            buf += ch\n",
    "        else:\n",
    "            clusters.append(buf)\n",
    "            buf = ch\n",
    "    if buf:\n",
    "        clusters.append(buf)\n",
    "    return clusters\n",
    "\n",
    "# TODO: demo\n",
    "split_graphemes_basic('naïve café e\\u0301 🇺🇸')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12 — Visible Width Estimator (Monospace Heuristic)\n",
    "Implement `display_width(s)` that estimates width in a monospace terminal:\n",
    "- Treat East Asian *wide* (`W`) and *full-width* (`F`) characters as width 2.\n",
    "- Treat combining marks as width 0.\n",
    "- Everything else as width 1.\n",
    "\n",
    "Use `unicodedata.east_asian_width(ch)` and `unicodedata.combining(ch)`.\n",
    "\n",
    "Test with mixed ASCII, CJK (`'漢字'`), and emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 1, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_width(s: str) -> int:\n",
    "    width = 0\n",
    "    for ch in s:\n",
    "        if _ud.combining(ch):\n",
    "            continue\n",
    "        eaw = _ud.east_asian_width(ch)\n",
    "        if eaw in ('W', 'F'):\n",
    "            width += 2\n",
    "        else:\n",
    "            width += 1\n",
    "    return width\n",
    "\n",
    "# TODO: demos\n",
    "display_width('Hello'), display_width('漢字'), display_width('Á'), display_width('😀')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
